# coding=utf-8

"""
Kafka å®¢æˆ·ç«¯

æä¾› Kafka ç”Ÿäº§è€…çš„å°è£…
"""
import json
import logging
from typing import Optional, Dict, Any
from datetime import datetime

try:
    from kafka import KafkaProducer, KafkaAdminClient
    from kafka.admin import NewTopic
    from kafka.errors import KafkaError, TopicAlreadyExistsError
    KAFKA_AVAILABLE = True
except ImportError as e:
    KAFKA_AVAILABLE = False
    logging.warning(f"kafka-python æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ï¼ŒKafka åŠŸèƒ½å°†ä¸å¯ç”¨: {e}")
except Exception as e:
    KAFKA_AVAILABLE = False
    logging.warning(f"kafka-python å¯¼å…¥æ—¶å‘ç”Ÿé”™è¯¯ï¼ŒKafka åŠŸèƒ½å°†ä¸å¯ç”¨: {e}")


class KafkaClient:
    """Kafka å®¢æˆ·ç«¯å°è£…ç±»"""
    
    def __init__(
        self,
        bootstrap_servers: str = "localhost:10109",
        enable_kafka: bool = False
    ):
        """
        åˆå§‹åŒ– Kafka å®¢æˆ·ç«¯
        
        Args:
            bootstrap_servers: Kafka æœåŠ¡å™¨åœ°å€
                - Docker å†…éƒ¨: kafka:9092
                - å®¿ä¸»æœºå¤–éƒ¨: localhost:10109 (æ ¹æ® docker-compose.yml é…ç½®)
            enable_kafka: æ˜¯å¦å¯ç”¨ Kafka
        """
        self.bootstrap_servers = bootstrap_servers
        self.enable_kafka = enable_kafka and KAFKA_AVAILABLE
        self.producer: Optional[KafkaProducer] = None
        self.admin_client: Optional[KafkaAdminClient] = None
        self.logger = logging.getLogger(__name__)
        
        # ç¦ç”¨ kafka-python åº“çš„è¯¦ç»†æ—¥å¿—
        if self.enable_kafka:
            logging.getLogger('kafka').setLevel(logging.WARNING)
            logging.getLogger('kafka.conn').setLevel(logging.WARNING)
            logging.getLogger('kafka.coordinator').setLevel(logging.WARNING)
            logging.getLogger('kafka.consumer').setLevel(logging.WARNING)
            logging.getLogger('kafka.cluster').setLevel(logging.WARNING)
        
        if self.enable_kafka:
            try:
                # å…ˆåˆ›å»º AdminClient ç”¨äºæ£€æŸ¥å’Œç®¡ç† topics
                self.admin_client = KafkaAdminClient(
                    bootstrap_servers=bootstrap_servers,
                    client_id='trendradar-admin',
                    request_timeout_ms=10000,
                )
                
                # åˆ›å»º Producer
                self.producer = KafkaProducer(
                    bootstrap_servers=bootstrap_servers,
                    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),
                    key_serializer=lambda k: k.encode('utf-8') if k and isinstance(k, str) else (k if k else None),
                    request_timeout_ms=30000,
                    retries=3,
                )
                self.logger.info(f"âœ… Kafka è¿æ¥æˆåŠŸ: {bootstrap_servers}")
            except Exception as e:
                self.logger.error(f"âŒ Kafka ç”Ÿäº§è€…åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_kafka = False
        else:
            if not KAFKA_AVAILABLE:
                self.logger.warning("âš ï¸  kafka-python æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install kafka-python")
            else:
                self.logger.info("â„¹ï¸  Kafka åŠŸèƒ½å·²ç¦ç”¨")
    
    def ensure_topic_exists(
        self,
        topic: str,
        num_partitions: int = 3,
        replication_factor: int = 1
    ) -> bool:
        """
        æ£€æŸ¥ topic æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        
        Args:
            topic: Topic åç§°
            num_partitions: åˆ†åŒºæ•°ï¼ˆé»˜è®¤3ï¼‰
            replication_factor: å‰¯æœ¬æ•°ï¼ˆé»˜è®¤1ï¼Œå•èŠ‚ç‚¹ï¼‰
        
        Returns:
            bool: topic æ˜¯å¦å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ
        """
        if not self.enable_kafka or not self.admin_client:
            return False
        
        try:
            # æ£€æŸ¥ topic æ˜¯å¦å­˜åœ¨ï¼šåˆ—å‡ºæ‰€æœ‰ topics
            existing_topics = self.admin_client.list_topics(timeout_ms=5000)
            
            if topic in existing_topics:
                self.logger.debug(f"âœ… Topic '{topic}' å·²å­˜åœ¨")
                return True
        except Exception as e:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥åˆ›å»ºï¼ˆå¯èƒ½ topic ä¸å­˜åœ¨æˆ–è¿æ¥é—®é¢˜ï¼‰
            self.logger.debug(f"æ£€æŸ¥ topic æ—¶å‡ºé”™ï¼Œå°è¯•åˆ›å»º: {e}")
        
        # Topic ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
        try:
            topic_list = [
                NewTopic(
                    name=topic,
                    num_partitions=num_partitions,
                    replication_factor=replication_factor
                )
            ]
            self.admin_client.create_topics(new_topics=topic_list, validate_only=False)
            self.logger.debug(f"âœ… å·²åˆ›å»º Topic '{topic}' (partitions={num_partitions}, replication={replication_factor})")
            return True
        except TopicAlreadyExistsError:
            # å¹¶å‘åˆ›å»ºæ—¶å¯èƒ½å·²å­˜åœ¨
            self.logger.debug(f"âœ… Topic '{topic}' å·²å­˜åœ¨ï¼ˆå¹¶å‘åˆ›å»ºï¼‰")
            return True
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»º Topic '{topic}' å¤±è´¥: {e}")
            # å³ä½¿åˆ›å»ºå¤±è´¥ï¼Œä¹Ÿå°è¯•ç»§ç»­ï¼ˆå¯èƒ½ä¾èµ–è‡ªåŠ¨åˆ›å»ºï¼‰
            return False
    
    def send(
        self,
        topic: str,
        data: Dict[str, Any],
        key: Optional[str] = None,
        ensure_topic: bool = True,
        headers: Optional[Dict[str, str]] = None
    ) -> bool:
        """
        å‘é€æ•°æ®åˆ° Kafka
        
        Args:
            topic: Kafka topic åç§°
            data: æ•°æ®å­—å…¸
            key: æ¶ˆæ¯é”®ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†åŒºï¼‰
            ensure_topic: æ˜¯å¦ç¡®ä¿ topic å­˜åœ¨
            headers: æ¶ˆæ¯ headersï¼ˆå¦‚ event_typeï¼‰
        
        Returns:
            bool: æ˜¯å¦å‘é€æˆåŠŸ
        """
        if not self.enable_kafka or not self.producer:
            return False
        
        # ç¡®ä¿ topic å­˜åœ¨
        if ensure_topic:
            if not self.ensure_topic_exists(topic):
                self.logger.warning(f"âš ï¸  Topic '{topic}' ä¸å­˜åœ¨ä¸”åˆ›å»ºå¤±è´¥ï¼Œå°è¯•ç›´æ¥å‘é€ï¼ˆå¯èƒ½ä¾èµ–è‡ªåŠ¨åˆ›å»ºï¼‰")
        
        try:
            # ç¡®ä¿ data æ˜¯å­—å…¸æ ¼å¼
            if not isinstance(data, dict):
                raise ValueError(f"data å¿…é¡»æ˜¯å­—å…¸ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(data)}")
            
            # æ·»åŠ æ—¶é—´æˆ³
            if '_timestamp' not in data:
                data['_timestamp'] = datetime.now().isoformat()
            
            # æ„å»º headersï¼ˆkafka-python æ ¼å¼: [(key_str, value_bytes), ...]ï¼‰
            kafka_headers = []
            if headers:
                for k, v in headers.items():
                    # kafka-python è¦æ±‚: key å¿…é¡»æ˜¯ str, value å¿…é¡»æ˜¯ bytes
                    kafka_headers.append((str(k), v.encode('utf-8') if isinstance(v, str) else v))
            
            # å‡†å¤‡ keyï¼ˆè®© key_serializer å¤„ç†ç¼–ç ï¼‰
            # å¦‚æœ key æ˜¯ bytesï¼Œå…ˆè½¬æ¢ä¸º strï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
            if key:
                kafka_key = key.decode('utf-8') if isinstance(key, bytes) else str(key)
            else:
                kafka_key = None
            
            future = self.producer.send(
                topic=topic,
                key=kafka_key,  # ä¼ å…¥ strï¼Œè®© key_serializer å¤„ç†ç¼–ç 
                value=data,  # value_serializer ä¼šè‡ªåŠ¨å¤„ç†åºåˆ—åŒ–
                headers=kafka_headers if kafka_headers else None
            )
            
            # ç­‰å¾…å‘é€ç»“æœ
            record_metadata = future.get(timeout=10)
            self.logger.debug(
                f"ğŸ“¤ æ¶ˆæ¯å·²å‘é€åˆ° topic={record_metadata.topic}, "
                f"partition={record_metadata.partition}, "
                f"offset={record_metadata.offset}"
            )
            return True
        except KafkaError as e:
            error_msg = str(e) if e else "æœªçŸ¥ Kafka é”™è¯¯"
            self.logger.error(f"âŒ Kafka å‘é€å¤±è´¥: {error_msg}", exc_info=True)
            return False
        except Exception as e:
            error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
            error_type = type(e).__name__
            self.logger.error(f"âŒ å‘é€æ¶ˆæ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ [{error_type}]: {error_msg}", exc_info=True)
            return False
    
    def send_batch(
        self,
        topic: str,
        data_list: list[Dict[str, Any]],
        key_prefix: Optional[str] = None,
        ensure_topic: bool = True,
        headers: Optional[Dict[str, str]] = None
    ) -> int:
        """
        æ‰¹é‡å‘é€æ•°æ®
        
        Args:
            topic: Kafka topic åç§°
            data_list: æ•°æ®åˆ—è¡¨
            key_prefix: æ¶ˆæ¯é”®å‰ç¼€
            ensure_topic: æ˜¯å¦ç¡®ä¿ topic å­˜åœ¨
            headers: æ¶ˆæ¯ headersï¼ˆå¦‚ event_typeï¼‰ï¼Œæ‰€æœ‰æ¶ˆæ¯å…±ç”¨
        
        Returns:
            int: æˆåŠŸå‘é€çš„æ•°é‡
        """
        if not self.enable_kafka or not self.producer:
            return 0
        
        # ç¡®ä¿ topic å­˜åœ¨ï¼ˆåªæ£€æŸ¥ä¸€æ¬¡ï¼‰
        if ensure_topic:
            if not self.ensure_topic_exists(topic):
                self.logger.warning(f"âš ï¸  Topic '{topic}' ä¸å­˜åœ¨ä¸”åˆ›å»ºå¤±è´¥ï¼Œå°è¯•ç›´æ¥å‘é€ï¼ˆå¯èƒ½ä¾èµ–è‡ªåŠ¨åˆ›å»ºï¼‰")
        
        success_count = 0
        for idx, data in enumerate(data_list):
            key = f"{key_prefix}_{idx}" if key_prefix else str(idx)
            # æ‰¹é‡å‘é€æ—¶ï¼Œåªåœ¨ç¬¬ä¸€æ¬¡æ£€æŸ¥ topicï¼Œåç»­ä¸å†æ£€æŸ¥
            if self.send(topic, data, key, ensure_topic=(idx == 0 and ensure_topic), headers=headers):
                success_count += 1
        
        # ç¡®ä¿æ‰€æœ‰æ¶ˆæ¯éƒ½å‘é€å®Œæˆ
        if success_count > 0:
            self.producer.flush()
        
        self.logger.debug(f"ğŸ“¤ æ‰¹é‡å‘é€å®Œæˆ: {success_count}/{len(data_list)}")
        return success_count
    
    def close(self):
        """å…³é—­ Kafka è¿æ¥"""
        if self.admin_client:
            try:
                self.admin_client.close()
                self.logger.debug("âœ… Kafka AdminClient å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"âŒ å…³é—­ Kafka AdminClient æ—¶å‡ºé”™: {e}")
        
        if self.producer:
            try:
                self.producer.close()
                self.logger.debug("âœ… Kafka ç”Ÿäº§è€…å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"âŒ å…³é—­ Kafka ç”Ÿäº§è€…æ—¶å‡ºé”™: {e}")

